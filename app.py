# app.py
# Streamlit App: Zomato Bangalore Restaurants â€” EDA + ML Classification + Neural Network
# Author: Generated by ChatGPT (updated to harden Keras usage on Streamlit Cloud)
# Notes:
# - Place the dataset CSV (e.g., "zomato.csv") in the same folder OR upload it from the sidebar.
# - If deploying to Streamlit Cloud and the dataset is too large, enable sampling from the sidebar.
# - Trains on first run and caches results to speed up subsequent runs.
# - Target variable: "online_order" (Yes/No). You can switch to "book_table" in the sidebar if desired.
# - Update (2025-08-14): Force NumPy arrays (float32) for Keras, add safe fallback to sklearn-MLP if Keras backend
#   raises errors (e.g., optree / JAX issues); add sidebar toggle to disable Keras entirely.

import io
import os
import re
import numpy as np
import pandas as pd
import plotly.express as px
import streamlit as st

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.neural_network import MLPClassifier

import joblib
import warnings
warnings.filterwarnings("ignore")

# Optional: Try importing TensorFlow Keras; fall back to sklearn MLP if unavailable.
USE_KERAS = True
KERAS_AVAILABLE = False
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Dropout
    from tensorflow.keras.callbacks import EarlyStopping
    # make sure default dtype is float32 for stability across backends
    try:
        tf.keras.backend.set_floatx('float32')
    except Exception:
        pass
    KERAS_AVAILABLE = True
except Exception:
    USE_KERAS = False
    KERAS_AVAILABLE = False

st.set_page_config(page_title="Zomato Bangalore ML & NN", layout="wide")

# -----------------------------
# Sidebar: Data Source & Options
# -----------------------------
st.sidebar.title("Settings")

st.sidebar.markdown("**Dataset Source**")
uploaded = st.sidebar.file_uploader("Upload dataset CSV (optional)", type=["csv"])

# Avoid hardcoded local Windows path defaults on cloud; only use if exists
default_path = r"C:\Users\ZBook\Desktop\juypter\projectFinal\zomato.csv"
data_path = uploaded if uploaded is not None else (default_path if os.path.exists(default_path) else None)

st.sidebar.markdown("---")
st.sidebar.markdown("---")
st.sidebar.markdown("**Deployment Tip**: If the dataset is too large, enable sampling.")
sample_frac = st.sidebar.slider("Sample fraction for training (use 0.01â€“0.02 on Streamlit Cloud for speed)", 0.01, 1.0, 0.02, 0.01)

target_col = st.sidebar.selectbox("Choose target (classification)", ["online_order", "book_table"])
random_state = st.sidebar.number_input("Random state (reproducibility)", value=42, step=1)

with st.sidebar.expander("Advanced Training Options"):
    test_size = st.slider("Test size", 0.1, 0.4, 0.2, 0.05)
    use_xgb = st.checkbox("Try XGBoost (if installed)", value=True)
    n_estimators = st.number_input("RandomForest n_estimators", 50, 600, 200, 50)
    rf_max_depth = st.select_slider("RandomForest max_depth", options=[None, 5, 10, 15, 20, 30], value=None)
    gb_learning_rate = st.number_input("GradientBoosting learning_rate", 0.01, 0.5, 0.1, 0.01, format="%.2f")
    mlp_hidden = st.text_input("sklearn-MLP hidden_layers (comma-separated)", value="128,64")
    keras_epochs = st.number_input("Keras NN epochs", min_value=5, max_value=100, value=25, step=5)
    keras_batch = st.number_input("Keras NN batch_size", min_value=16, max_value=512, value=128, step=16)

# Allow disabling Keras entirely (recommended on Streamlit Cloud if you see backend errors)
if KERAS_AVAILABLE:
    use_keras_toggle = st.sidebar.checkbox("Use Keras neural network (disable if errors occur)", value=False)
    USE_KERAS = bool(use_keras_toggle)
else:
    USE_KERAS = False

st.title("ðŸ½ï¸ Zomato Bangalore â€” EDA, ML Classification & Neural Network")

# -----------------------------
# Utility functions
# -----------------------------
def clean_rate(val):
    # Convert ratings like "3.9/5", "NEW", "-" to float
    if pd.isna(val):
        return np.nan
    s = str(val).strip()
    if s in ["NEW", "-", "nan", "NaN", ""]:
        return np.nan
    s = s.replace("/5", "").strip()
    try:
        return float(s)
    except:
        return np.nan

def clean_cost(val):
    # "1,200" -> 1200
    if pd.isna(val):
        return np.nan
    s = re.sub(r"[^\d.]", "", str(val))
    return float(s) if s else np.nan

def yesno_to_int(s):
    if pd.isna(s):
        return np.nan
    s = str(s).strip().lower()
    if s in ["yes", "y", "true", "1"]:
        return 1
    if s in ["no", "n", "false", "0"]:
        return 0
    return np.nan

def load_dataframe(_path_or_buf):
    if _path_or_buf is None:
        st.warning("Please upload the dataset CSV from the sidebar or place 'zomato.csv' in the app folder.")
        st.stop()
    if hasattr(_path_or_buf, "read"):
        return pd.read_csv(_path_or_buf, encoding="utf-8")
    return pd.read_csv(_path_or_buf, encoding="utf-8")

# -----------------------------
# Load data
# -----------------------------
df_raw = load_dataframe(data_path)
st.success(f"Loaded dataset with shape: {df_raw.shape}")

# -----------------------------
# Light cleaning & feature selection
# -----------------------------
df = df_raw.copy()

# Standardize column names (strip + lower) â€” keep original case if already proper
df.columns = [c.strip() for c in df.columns]

# Known columns from the Kaggle dataset; keep a relevant subset
drop_cols = [
    'url', 'address', 'phone', 'name', 'reviews_list', 'menu_item',
    'listed_in(type)', 'listed_in(city)'  # keep or drop depending on experiments
]
for c in drop_cols:
    if c not in df.columns:
        # do not error if missing in this particular copy
        pass

# Convert key fields
if 'rate' in df.columns:
    df['rate'] = df['rate'].apply(clean_rate)
if 'votes' in df.columns:
    df['votes'] = pd.to_numeric(df['votes'], errors='coerce')
if 'approx_cost(for two people)' in df.columns:
    df['approx_cost(for two people)'] = df['approx_cost(for two people)'].apply(clean_cost)
elif 'approx_cost(for_two_people)' in df.columns:
    df['approx_cost(for_two_people)'] = df['approx_cost(for_two_people)'].apply(clean_cost)

# Normalize target columns to {0,1}
if 'online_order' in df.columns:
    df['online_order'] = df['online_order'].apply(yesno_to_int)
if 'book_table' in df.columns:
    df['book_table'] = df['book_table'].apply(yesno_to_int)

# Fill some obvious missing values
for col in ['rate', 'votes']:
    if col in df.columns:
        df[col] = df[col].fillna(df[col].median())

# Cost column might have 2 different names in different copies
cost_col = None
if 'approx_cost(for two people)' in df.columns:
    cost_col = 'approx_cost(for two people)'
elif 'approx_cost(for_two_people)' in df.columns:
    cost_col = 'approx_cost(for_two_people)'

if cost_col:
    df[cost_col] = df[cost_col].fillna(df[cost_col].median())

# Categorical columns commonly used
cat_cols = [c for c in ['location', 'rest_type', 'cuisines'] if c in df.columns]
# 'listed_in(type)' and 'listed_in(city)' can be high-cardinality; include if present
for c in ['listed_in(type)', 'listed_in(city)']:
    if c in df.columns:
        cat_cols.append(c)

num_cols = [c for c in ['rate', 'votes', cost_col] if c and c in df.columns]

# Final feature set
feature_cols = num_cols + cat_cols
feature_cols = [c for c in feature_cols if c != target_col and c in df.columns]
df_model = df[feature_cols + [target_col]].dropna(subset=[target_col])
df_model = df_model.sample(frac=float(sample_frac), random_state=random_state) if sample_frac < 1.0 else df_model

st.info(f"Using {len(feature_cols)} features: {feature_cols}")
st.write(f"Training rows after sampling: **{df_model.shape[0]}**")

# -----------------------------
# EDA Section (Page 1)
# -----------------------------
eda_tab, models_tab, predict_tab = st.tabs(["ðŸ“Š Analysis (EDA)", "ðŸ¤– Train & Compare Models", "ðŸ§  Prediction"])

with eda_tab:
    st.subheader("Basic Overview")
    st.write(df_model.describe(include='all').transpose())

    if 'rate' in df_model.columns:
        fig = px.histogram(df_model, x='rate', nbins=30, title='Distribution of Ratings')
        st.plotly_chart(fig, use_container_width=True)

    if cost_col and cost_col in df_model.columns:
        fig = px.histogram(df_model, x=cost_col, nbins=40, title='Approx. Cost for Two (Distribution)')
        st.plotly_chart(fig, use_container_width=True)

    if 'votes' in df_model.columns:
        fig = px.histogram(df_model, x='votes', nbins=40, title='Votes (Distribution)')
        st.plotly_chart(fig, use_container_width=True)

    if 'location' in df_model.columns:
        top_locs = df_model['location'].value_counts().nlargest(15).reset_index()
        top_locs.columns = ['location', 'count']
        fig = px.bar(top_locs, x='location', y='count', title='Top 15 Locations by Count')
        st.plotly_chart(fig, use_container_width=True)

    if 'rest_type' in df_model.columns:
        top_rest = df_model['rest_type'].value_counts().nlargest(15).reset_index()
        top_rest.columns = ['rest_type', 'count']
        fig = px.bar(top_rest, x='rest_type', y='count', title='Top 15 Restaurant Types')
        st.plotly_chart(fig, use_container_width=True)

    st.markdown("### Key Questions & Quick Answers")
    bullets = []
    # Example quick answers (adapt to available columns)
    if 'rate' in df_model.columns:
        bullets.append(f"**Average rating:** {df_model['rate'].mean():.2f}")
    if cost_col and cost_col in df_model.columns:
        bullets.append(f"**Median cost for two:** {df_model[cost_col].median():.0f}")
    if 'online_order' in df_model.columns:
        pct = 100 * df_model['online_order'].mean()
        bullets.append(f"**% restaurants offering online ordering (in sample):** {pct:.1f}%")
    if 'book_table' in df_model.columns:
        pct = 100 * df_model['book_table'].mean()
        bullets.append(f"**% restaurants allowing table booking (in sample):** {pct:.1f}%")
    if bullets:
        st.markdown("- " + "\n- ".join(bullets))

# -----------------------------
# ML Utilities
# -----------------------------
def build_preprocessor(numeric_cols, categorical_cols):
    numeric_transformer = Pipeline(steps=[
        ("scaler", StandardScaler())
    ])
    # Handle sklearn versions: prefer new arg, fall back to old if needed
    try:
        categorical_transformer = Pipeline(steps=[
            ("ohe", OneHotEncoder(handle_unknown="ignore", sparse_output=False))
        ])
    except TypeError:
        # sklearn <1.2
        categorical_transformer = Pipeline(steps=[
            ("ohe", OneHotEncoder(handle_unknown="ignore", sparse=False))
        ])
    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_cols),
            ("cat", categorical_transformer, categorical_cols),
        ],
        remainder="drop"
    )
    return preprocessor

def train_ml_models(X, y):
    results = {}

    # Logistic Regression
    log_reg = Pipeline(steps=[
        ("pre", build_preprocessor(num_cols, cat_cols)),
        ("clf", LogisticRegression(max_iter=200))
    ])
    log_reg.fit(X, y)
    results["LogisticRegression"] = log_reg

    # Random Forest
    rf = Pipeline(steps=[
        ("pre", build_preprocessor(num_cols, cat_cols)),
        ("clf", RandomForestClassifier(n_estimators=int(n_estimators), max_depth=rf_max_depth, random_state=random_state))
    ])
    rf.fit(X, y)
    results["RandomForest"] = rf

    # Gradient Boosting (robust, lighter than XGB)
    gb = Pipeline(steps=[
        ("pre", build_preprocessor(num_cols, cat_cols)),
        ("clf", GradientBoostingClassifier(learning_rate=float(gb_learning_rate), random_state=random_state))
    ])
    gb.fit(X, y)
    results["GradientBoosting"] = gb

    # Optional: XGBoost
    if use_xgb:
        try:
            from xgboost import XGBClassifier
            xgb = Pipeline(steps=[
                ("pre", build_preprocessor(num_cols, cat_cols)),
                ("clf", XGBClassifier(
                    n_estimators=300,
                    max_depth=6,
                    learning_rate=0.1,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    eval_metric="logloss",
                    random_state=random_state,
                    tree_method="hist"
                ))
            ])
            xgb.fit(X, y)
            results["XGBoost"] = xgb
        except Exception as e:
            st.warning(f"XGBoost not available or failed to train: {e}")

    return results

def evaluate_models(models, X_test, y_test):
    rows = []
    reports = {}
    cms = {}
    for name, pipe in models.items():
        pred = pipe.predict(X_test)
        acc = accuracy_score(y_test, pred)
        rows.append({"Model": name, "Accuracy": acc})
        try:
            reports[name] = classification_report(y_test, pred, output_dict=False)
        except Exception:
            reports[name] = "N/A"
        cms[name] = confusion_matrix(y_test, pred)
    summary = pd.DataFrame(rows).sort_values("Accuracy", ascending=False).reset_index(drop=True)
    return summary, reports, cms

# Neural Network (Keras) â€” we'll train on the preprocessed features from the best preprocessor
def build_keras_model(input_dim: int):
    model = Sequential([
        Dense(256, activation='relu', input_shape=(input_dim,)),
        Dropout(0.3),
        Dense(128, activation='relu'),
        Dropout(0.2),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

@st.cache_resource(show_spinner=True)
def run_training(df_model, target_col, test_size, seed):
    # Split
    X = df_model[feature_cols]
    y = df_model[target_col].astype(int)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=float(test_size), random_state=seed, stratify=y)

    # Train classic ML
    ml_models = train_ml_models(X_train, y_train)
    ml_summary, ml_reports, ml_cms = evaluate_models(ml_models, X_test, y_test)

    # Pick best ML
    best_name = ml_summary.iloc[0]['Model']
    best_ml = ml_models[best_name]

    # Prepare arrays for NN: fit the same preprocessor as best ML
    pre = best_ml.named_steps['pre']
    # ensure fitted (pipe was fitted already, but safe to refit on train portion)
    X_train_pre = pre.fit_transform(X_train)
    X_test_pre = pre.transform(X_test)

    # Force NumPy float32 (fixes Keras/JAX/optree issues on some cloud envs)
    X_train_np = np.asarray(X_train_pre, dtype=np.float32)
    X_test_np = np.asarray(X_test_pre, dtype=np.float32)
    y_train_np = y_train.to_numpy().astype(np.float32)  # binary labels as float32
    y_test_np = y_test.to_numpy().astype(np.float32)

    # Two NN options
    nn_info = {}
    if USE_KERAS:
        try:
            input_dim = X_train_np.shape[1]
            model = build_keras_model(input_dim)
            es = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)
            _ = model.fit(
                X_train_np, y_train_np,
                validation_split=0.2,
                epochs=int(keras_epochs),
                batch_size=int(keras_batch),
                verbose=0,
                callbacks=[es]
            )
            nn_metrics = model.evaluate(X_test_np, y_test_np, verbose=0)
            nn_acc = float(nn_metrics[1]) if isinstance(nn_metrics, (list, tuple)) else float(nn_metrics)
            nn_info["type"] = "Keras"
            nn_info["model"] = model
            nn_info["pre"] = pre
            nn_info["test_acc"] = nn_acc
        except Exception as e:
            st.warning(f"Keras NN failed (falling back to sklearn MLP): {e}")
            # Fallback: sklearn MLPClassifier
            hidden = tuple(int(x) for x in re.split(r\"\\s*,\\s*\", mlp_hidden.strip()) if x)
            mlp = MLPClassifier(hidden_layer_sizes=hidden, max_iter=200, random_state=seed)
            nn_pipe = Pipeline(steps=[(\"pre\", pre), (\"clf\", mlp)])
            nn_pipe.fit(X_train, y_train)
            nn_pred = nn_pipe.predict(X_test)
            nn_acc = accuracy_score(y_test, nn_pred)
            nn_info[\"type\"] = \"sklearn-MLP\"
            nn_info[\"model\"] = nn_pipe
            nn_info[\"pre\"] = pre
            nn_info[\"test_acc\"] = float(nn_acc)
    else:
        # Fallback: sklearn MLPClassifier
        hidden = tuple(int(x) for x in re.split(r\"\\s*,\\s*\", mlp_hidden.strip()) if x)
        mlp = MLPClassifier(hidden_layer_sizes=hidden, max_iter=200, random_state=seed)
        nn_pipe = Pipeline(steps=[(\"pre\", pre), (\"clf\", mlp)])
        nn_pipe.fit(X_train, y_train)
        nn_pred = nn_pipe.predict(X_test)
        nn_acc = accuracy_score(y_test, nn_pred)
        nn_info[\"type\"] = \"sklearn-MLP\"
        nn_info[\"model\"] = nn_pipe
        nn_info[\"pre\"] = pre
        nn_info[\"test_acc\"] = float(nn_acc)

    # Persist artifacts locally (optional)
    try:
        os.makedirs(\"artifacts\", exist_ok=True)
        joblib.dump(best_ml, \"artifacts/best_ml_model.joblib\")
        joblib.dump(pre, \"artifacts/preprocessor.joblib\")
        if nn_info[\"type\"] == \"Keras\":
            # Save Keras model
            nn_info[\"model\"].save(\"artifacts/nn_keras_model.keras\")
        else:
            joblib.dump(nn_info[\"model\"], \"artifacts/nn_sklearn_mlp.joblib\")
    except Exception as e:
        st.warning(f\"Could not save artifacts: {e}\")

    return {
        \"X_train\": X_train, \"X_test\": X_test,
        \"y_train\": y_train, \"y_test\": y_test,
        \"ml_models\": ml_models,
        \"ml_summary\": ml_summary,
        \"ml_reports\": ml_reports,
        \"ml_cms\": ml_cms,
        \"best_ml_name\": best_name,
        \"best_ml\": best_ml,
        \"nn_info\": nn_info
    }

with models_tab:
    st.subheader(\"Train & Compare Models\")

    if target_col not in df_model.columns:
        st.error(f\"Target column '{target_col}' not found. Please choose another target.\")
        st.stop()

    train_button = st.button(\"ðŸš€ Train / Retrain models now\", type=\"primary\")

    if train_button or \"train_result\" not in st.session_state:
        with st.spinner(\"Training models... This may take a minute on first run.\"):
            st.session_state[\"train_result\"] = run_training(df_model, target_col, test_size, random_state)

    if \"train_result\" in st.session_state:
        result = st.session_state[\"train_result\"]
        st.success(\"Training complete. See comparison below:\")
        st.dataframe(result[\"ml_summary\"], use_container_width=True)

        best_ml_name = result[\"best_ml_name\"]
        st.info(f\"**Best ML model:** {best_ml_name} (highest accuracy in this run)\")

        # Show simple reports
        with st.expander(\"Classification Reports\"):
            for name, rpt in result[\"ml_reports\"].items():
                st.markdown(f\"**{name}**\")
                st.text(rpt)

        with st.expander(\"Confusion Matrices\"):
            for name, cm in result[\"ml_cms\"].items():
                st.markdown(f\"**{name}**\")
                st.write(pd.DataFrame(cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Pred 0\", \"Pred 1\"]))

        # NN
        st.markdown(\"---\")
        st.subheader(\"Neural Network Result\")
        nn_info = result[\"nn_info\"]
        st.write(f\"NN Type: **{nn_info['type']}**, Test Accuracy: **{nn_info['test_acc']:.4f}**\")

# -----------------------------
# Prediction Page
# -----------------------------
with predict_tab:
    st.subheader(\"Predict on a New Restaurant\")

    # Input fields for a single example
    col1, col2 = st.columns(2)
    with col1:
        in_rate = st.number_input(\"Average Rating (0â€“5)\", min_value=0.0, max_value=5.0, value=3.8, step=0.1)
        in_votes = st.number_input(\"Votes\", min_value=0, max_value=50000, value=150, step=10)
        in_cost = st.number_input(\"Approx Cost for Two\", min_value=0, max_value=100000, value=800, step=50)
    with col2:
        in_location = st.text_input(\"Location (e.g., BTM, Indiranagar)\", value=\"BTM\")
        in_rest_type = st.text_input(\"Restaurant Type (e.g., Casual Dining)\", value=\"Casual Dining\")
        in_cuisines = st.text_input(\"Cuisines (e.g., North Indian, Chinese)\", value=\"North Indian, Chinese\")
        if 'listed_in(type)' in df.columns:
            in_listed_type = st.text_input(\"Listed In (type)\", value=\"Delivery\")
        else:
            in_listed_type = None
        if 'listed_in(city)' in df.columns:
            in_listed_city = st.text_input(\"Listed In (city)\", value=\"Banashankari\")
        else:
            in_listed_city = None

    # Build a single-row DataFrame with correct feature columns
    def build_input_df():
        row = {
            \"rate\": in_rate if \"rate\" in feature_cols else np.nan,
            \"votes\": in_votes if \"votes\" in feature_cols else np.nan,
        }
        if cost_col and cost_col in feature_cols:
            row[cost_col] = in_cost

        if \"location\" in feature_cols:
            row[\"location\"] = in_location
        if \"rest_type\" in feature_cols:
            row[\"rest_type\"] = in_rest_type
        if \"cuisines\" in feature_cols:
            row[\"cuisines\"] = in_cuisines
        if \"listed_in(type)\" in feature_cols and in_listed_type is not None:
            row[\"listed_in(type)\"] = in_listed_type
        if \"listed_in(city)\" in feature_cols and in_listed_city is not None:
            row[\"listed_in(city)\"] = in_listed_city
        return pd.DataFrame([row])

    st.markdown(\"When ready, click **Predict**.\")
    if st.button(\"ðŸ”® Predict\"):
        if \"train_result\" not in st.session_state:
            st.warning(\"Please train models first on the 'Train & Compare Models' tab.\")
        else:
            result = st.session_state[\"train_result\"]
            best_ml = result[\"best_ml\"]
            nn_info = result[\"nn_info\"]

            X_new = build_input_df()

            # Best ML Prediction
            try:
                ml_pred = best_ml.predict(X_new)[0]
                if hasattr(best_ml.named_steps['clf'], \"predict_proba\"):
                    ml_proba = best_ml.predict_proba(X_new)[0,1]
                else:
                    # Calibrate using decision_function if available
                    if hasattr(best_ml.named_steps['clf'], \"decision_function\"):
                        score = best_ml.named_steps['clf'].decision_function(X_new)
                        ml_proba = 1 / (1 + np.exp(-score))[0]
                    else:
                        ml_proba = np.nan
                st.success(f\"Best ML Model Prediction: **{int(ml_pred)}** (probability={ml_proba:.3f})\")
            except Exception as e:
                st.error(f\"Best ML prediction failed: {e}\")

            # NN Prediction
            try:
                if nn_info[\"type\"] == \"Keras\":
                    Xp = nn_info[\"pre\"].transform(X_new)
                    Xp = np.asarray(Xp, dtype=np.float32)
                    p = nn_info[\"model\"].predict(Xp, verbose=0).ravel()[0]
                    pred_nn = int(p >= 0.5)
                    st.info(f\"Neural Network Prediction: **{pred_nn}** (probability={p:.3f})\")
                else:
                    pred_nn = nn_info[\"model\"].predict(X_new)[0]
                    if hasattr(nn_info[\"model\"], \"predict_proba\"):
                        p = nn_info[\"model\"].predict_proba(X_new)[0,1]
                    else:
                        p = np.nan
                    st.info(f\"Neural Network (sklearn-MLP) Prediction: **{int(pred_nn)}** (probability={p:.3f})\")
            except Exception as e:
                st.error(f\"NN prediction failed: {e}\")

# -----------------------------
# How to Deploy (footer)
# -----------------------------
st.markdown(\"\"\"
---
### ðŸš€ Deployment on Streamlit Cloud (Quick Guide)
1. Create a public GitHub repo and add **app.py** and **requirements.txt** (and your dataset file, e.g., `zomato.csv` or a smaller sampled CSV).
2. Go to **share.streamlit.io** â†’ **New app** â†’ Select your repo and branch â†’ App file: `app.py` â†’ Deploy.
3. If the dataset is too big, enable sampling in the sidebar (e.g., `0.01` or `0.02`) so it trains quickly on the cloud.
4. Optional: Pre-train locally, then commit the `artifacts/` folder so the app loads trained models instantly.
\"\"\")